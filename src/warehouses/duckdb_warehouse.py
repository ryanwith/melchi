import duckdb
import datetime
from .abstract_warehouse import AbstractWarehouse

class DuckDBWarehouse(AbstractWarehouse):
    def __init__(self, config):
        super().__init__("duckdb")  # Initialize with the warehouse type
        self.config = config
        self.connection = None

    def connect(self):
        self.connection = duckdb.connect(self.config['database'])

    def disconnect(self):
        if self.connection:
            self.connection.close()

    def begin_transaction(self):
        self.connection.begin()

    def commit_transaction(self):
        self.connection.commit()

    def rollback_transaction(self):
        self.connection.rollback()

    def get_schema(self, table_info):
        result = self.connection.execute(f"PRAGMA table_info('{self.get_full_table_name(table_info)}')")
        return [(col[1], col[2], col[5]) for col in result.fetchall()]

    def create_table(self, table_info, schema):
        schema_name = table_info["schema"]
        table_name = table_info["table"]
        primary_keys = []
        column_definitions = []

        # create a schema if needed
        self.connection.execute(f"CREATE SCHEMA IF NOT EXISTS {schema_name}")

        # build a list of primary keys
        for column in schema:
            if column[2] == 'Y':
                primary_keys.append(column[0])

        for column in schema:
            definition = f"{column[0]} {column[1]}"

            # add PRIMARY KEY designator to the primary key column if there's one or fewer pks
            if column[2] == "Y" and len(primary_keys) < 2:
                definition += " PRIMARY KEY"
            column_definitions.append(definition)

        # create a composite primary key at the end if there are multiple
        if len(primary_keys) >= 2:
            column_definitions.append(f"PRIMARY KEY ({", ".join(primary_keys)})")   

        # Will used autogenerated keys from snowflake and bigquery if a primary key isn't set
        if len(primary_keys) == 0:
            pk_name = "MELCHI_ROW_ID"
            primary_keys.append(pk_name)
            column_definitions.append(f"{pk_name} VARCHAR PRIMARY KEY")

        create_table_query = f"CREATE TABLE {schema_name}.{table_name} ({", ".join(column_definitions)});"

        # create the actual table
        self.connection.execute(create_table_query)

        current_timestamp = datetime.datetime.now()

        update_logs = f"""INSERT INTO {self.config["cdc_metadata_schema"]}.table_info VALUES (
            '{schema_name}', '{table_name}', '{current_timestamp}', '{current_timestamp}', {self.convert_list_to_duckdb_syntax(primary_keys)}
        )"""

        self.connection.execute(update_logs)

    def get_data(self, table_name):
        result = self.connection.execute(f"SELECT * FROM {table_name};")
        return result.fetchall()

    def get_data_as_df(self, table_name):
        pass

    def insert_data(self, table_name, data):
        # Implementation for inserting data into DuckDB
        pass

    def setup_target_environment(self):
        self.connection.execute(f"CREATE SCHEMA IF NOT EXISTS {self.config["cdc_metadata_schema"]};")
        self.connection.execute(f"""
            CREATE TABLE IF NOT EXISTS {self.config["cdc_metadata_schema"]}.table_info
                (schema_name varchar, table_name varchar, created_at timestamp, updated_at timestamp, primary_keys varchar[], PRIMARY KEY (schema_name, table_name));
        """)

    def create_cdc_stream(self, table_info):
        pass

    def get_changes(self, table_info):
        pass

    def get_full_table_name(self, table_info):
        return f"{table_info["schema"]}.{table_info["table"]}"

    def get_stream_name(self, table_info):
        pass

    def sync_table(self, table_info, df):
        full_table_name = self.get_full_table_name(table_info)
        temp_table_name = table_info["table"] + "_melchi_cdc"
        self.connection.execute(f"CREATE OR REPLACE TEMP TABLE {temp_table_name} AS (SELECT * FROM df)")    
        print('created')        
        columns_to_insert = []
        for row in self.get_schema(table_info):
            columns_to_insert.append(row[0])
        formatted_columns = ", ".join(columns_to_insert)
        formatted_primary_keys = ", ".join(self.get_primary_keys(table_info))
        print('delete start')
        delete_sql_statement = f"""DELETE FROM {full_table_name}
            WHERE ({formatted_primary_keys}) IN 
            (
                SELECT {formatted_primary_keys}
                FROM {temp_table_name}
                WHERE melchi_metadata_action = 'DELETE'
            );
        """
        print(delete_sql_statement)
        double_check_query = f"""SELECT * from {full_table_name}  WHERE ({formatted_primary_keys}) IN 
            (
                SELECT {formatted_primary_keys}
                FROM {temp_table_name}
                WHERE melchi_metadata_action = 'DELETE'
            );"""
        print(self.connection.execute(double_check_query).fetchall())
        print('insert start')
        insert_sql_statement = f"""INSERT INTO {full_table_name}
            SELECT {formatted_columns} FROM {temp_table_name}
            WHERE melchi_metadata_action = 'INSERT'
        """
        
        self.connection.execute(delete_sql_statement)
        self.connection.execute(insert_sql_statement)
        # add update table info query

        # if primary_keys == ["MELCHI_ROW_ID"]:
        self.connection.execute(f"DROP TABLE {temp_table_name}")
            

    def convert_list_to_duckdb_syntax(self, standard_list):
        return f"[{", ".join(list(map(lambda item: f"'{item}'", standard_list)))}]"
    
    def get_primary_keys(self, table_info):
        columns = self.connection.execute(f"PRAGMA table_info('{self.get_full_table_name(table_info)}')").fetchall()
        primary_keys = []
        for row in columns:
            if row[5] == True:
                primary_keys.append(row[1])
        return primary_keys
    
    def cleanup_cdc_for_table(self, table_info):
        pass
    # def insert_cdc_query()