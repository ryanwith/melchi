import duckdb
import datetime
from .abstract_warehouse import AbstractWarehouse

class DuckDBWarehouse(AbstractWarehouse):
    def __init__(self, config):
        super().__init__("duckdb")  # Initialize with the warehouse type
        self.config = config
        self.connection = None

    def connect(self):
        self.connection = duckdb.connect(self.config['database'])

    def disconnect(self):
        if self.connection:
            self.connection.close()

    def begin_transaction(self):
        self.connection.begin()

    def commit_transaction(self):
        self.connection.commit()

    def rollback_transaction(self):
        self.connection.rollback()

    def get_schema(self, table_info):
        result = self.connection.execute(f"PRAGMA table_info('{self.get_full_table_name(table_info)}')")
        rows = []
        for row in result.fetchall():
            rows.append(self.format_schema_row(row))
        return rows

    def create_table(self, table_info, source_schema, target_schema):
        primary_keys = []
        # create a schema if needed
        self.connection.execute(f"CREATE SCHEMA IF NOT EXISTS {table_info["schema"]}")
        print(target_schema)
        # build a list of primary keys
        for column in target_schema:
            if column["primary_key"] == True:
                primary_keys.append(column["name"])

        # Will used autogenerated keys from snowflake and bigquery if a primary key isn't set
        # we do not actually set the primary keys in duckdb as it is too eager for enforcement in transactions
        if len(primary_keys) == 0:
            pk_name = "MELCHI_ROW_ID"
            target_schema.append({
                "name": f"{pk_name}",
                "type": "VARCHAR",
                "nullable": False,
                "default_value": None,
                "primary_key": True
            })
            primary_keys.append(pk_name)
        
        create_table_query = self.generate_create_table_statement(table_info, target_schema)

        # create the actual table
        self.connection.execute(create_table_query)
        current_timestamp = datetime.datetime.now()

        source_columns = []

        for column in source_schema:
            source_db = f"'{table_info["database"]}'"
            source_schema_name = f"'{table_info["schema"]}'"
            source_table = f"'{table_info["table"]}'"
            name = f"'{column["name"]}'"
            type = f"'{column["type"]}'"
            nullable = "TRUE" if column["nullable"] == True else "FALSE"
            default_value = f"'{column["default_value"]}'" if column["default_value"] else "NULL"
            primary_key = "TRUE" if column["primary_key"] == True else "FALSE"
            source_column_values = f"""(
                {source_db}, {source_schema_name}, {source_table}, {name}, {type}, {default_value}, {nullable}, {primary_key}
            )"""
            source_columns.append(source_column_values)

        update_logs = [
            f"""
            INSERT INTO {self.get_metadata_schema()}.captured_tables VALUES (
                '{table_info["schema"]}', '{table_info["table"]}', '{current_timestamp}', '{current_timestamp}', {self.convert_list_to_duckdb_syntax(primary_keys)}
            );"""
            ,
            f"""
            INSERT INTO {self.get_metadata_schema()}.source_columns VALUES
            {(",\n").join(source_columns)};
            """
            ]

        self.connection.execute("\n".join(update_logs))

    def get_data(self, table_name):
        result = self.connection.execute(f"SELECT * FROM {table_name};")
        return result.fetchall()

    def get_data_as_df(self, table_name):
        pass

    def insert_data(self, table_name, data):
        # Implementation for inserting data into DuckDB
        pass

    def setup_target_environment(self):
        self.connection.execute(f"CREATE SCHEMA IF NOT EXISTS {self.config["cdc_metadata_schema"]};")
        self.connection.execute(f"""
            CREATE TABLE IF NOT EXISTS {self.config["cdc_metadata_schema"]}.captured_tables
                (schema_name varchar, table_name varchar, created_at timestamp, updated_at timestamp, primary_keys varchar[]);
        """)
        self.connection.execute(f"""
            CREATE TABLE IF NOT EXISTS {self.config["cdc_metadata_schema"]}.source_columns (
            table_catalog varchar, table_schema varchar, table_name varchar, column_name varchar, data_type varchar, column_default varchar, is_nullable boolean, primary_key boolean
            );
        """)

    def create_cdc_stream(self, table_info):
        pass

    def get_changes(self, table_info):
        pass

    def get_full_table_name(self, table_info):
        return f"{table_info["schema"]}.{table_info["table"]}"

    def get_stream_name(self, table_info):
        pass

    def sync_table(self, table_info, df):
        full_table_name = self.get_full_table_name(table_info)
        temp_table_name = table_info["table"] + "_melchi_cdc"

        # somehow this grabs the dataframe but i am not sure how
        self.connection.execute(f"CREATE OR REPLACE TEMP TABLE {temp_table_name} AS (SELECT * FROM df)")    
        columns_to_insert = []
        for col in self.get_schema(table_info):
            columns_to_insert.append(col["name"])
        formatted_columns = ", ".join(columns_to_insert)
        formatted_primary_keys = ", ".join(self.get_primary_keys(table_info))
        delete_sql_statement = f"""DELETE FROM {full_table_name}
            WHERE ({formatted_primary_keys}) IN 
            (
                SELECT ({formatted_primary_keys})
                FROM {temp_table_name}
                WHERE melchi_metadata_action = 'DELETE'
            );
        """
        print(delete_sql_statement)
        insert_sql_statement = f"""INSERT INTO {full_table_name}
            SELECT {formatted_columns} FROM {temp_table_name}
            WHERE melchi_metadata_action = 'INSERT'
        """
        
        self.connection.execute(delete_sql_statement)
        self.connection.execute(insert_sql_statement)
        self.update_cdc_tracker(table_info)
        # add update table info query

        self.connection.execute(f"DROP TABLE {temp_table_name}")            

    def convert_list_to_duckdb_syntax(self, standard_list):
        return f"[{", ".join(list(map(lambda item: f"'{item}'", standard_list)))}]"
    
    def get_primary_keys(self, table_info):
        captured_tables = f"{self.get_metadata_schema()}.captured_tables"
        print(captured_tables)
        get_primary_keys_query = f"""
            SELECT primary_keys FROM {captured_tables}
                WHERE table_name = '{table_info["table"]}' and schema_name = '{table_info["schema"]}'
        """
        # get_primary_keys_query = f"""
        #     SELECT primary_keys FROM {captured_tables}
        #         WHERE table_name = '{table_info["table"]}' and schema_name = '{table_info["schema"]}'
        # """
        primary_keys = self.connection.execute(get_primary_keys_query).fetchone()[0]
        return primary_keys
    
    def cleanup_cdc_for_table(self, table_info):
        pass
    
    def update_cdc_tracker(self, table_info):
        where_clause = f"WHERE table_name = '{table_info["table"]}' and schema_name = '{table_info["schema"]}'"
        self.connection.execute(f"UPDATE {self.get_metadata_schema()}.captured_tables SET updated_at = current_timestamp {where_clause} ")

    def execute_query(self, query_text):
        self.connection.execute(query_text)

    def fetch_results(self, num = None):
        if num is None:
            results = self.connection.fetchall()
        elif num == 1:
            results = self.connection.fetchone()
        elif num > 1:
            results = self.connection.fetchall()
        else:
            raise ValueError(f"Invalid value for 'num': {num}. Expected None or a positive integer.")
        return results   
    
    def replace_existing_tables(self):
        replace_existing = self.config.get("replace_existing", False)
        if replace_existing == True:
            return True
        else:
            return False
        
    def format_schema_row(self, row):
        return {
            "name": row[1],
            "type": row[2],
            "nullable": True if row[3] == "TRUE" else False,
            "default_value": row[4],
            "primary_key": True if row[5] == "TRUE" else False,
        }
    
    def generate_create_table_statement(self, table_info, schema):
        if self.replace_existing_tables() == True:
            create_statement = f"CREATE OR REPLACE TABLE {self.get_full_table_name(table_info)} "
        else:
            create_statement = f"CREATE TABLE IF NOT EXISTS {self.get_full_table_name(table_info)} "
        column_statements = []
        for col in schema:
            column_statement = f"{col["name"]} {col["type"]}"
            column_statement += " NOT NULL" if col["nullable"] == False else ""
            column_statement += f" DEFAULT {col["default_value"]}" if col["default_value"] is not None else ""
            column_statements.append(column_statement)
        full_create_statement = f"{create_statement}({", ".join(column_statements)});"

        # installs and loads spatial extension if that's required for the table
        if self.contains_spatial(schema):
            return f"INSTALL spatial;\nLOAD spatial;\n{full_create_statement}"
    
    def contains_spatial(self, schema):
        for column in schema:
            if column["type"] == "GEOMETRY":
                return True
        return False

    def insert_df(self, table_info, df):
        pass